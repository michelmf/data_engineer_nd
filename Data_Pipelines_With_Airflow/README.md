# Data Pipelines with Airflow

This is the fifth project of the Data Engineer Nanodegree. In this project, students need to implement an end-to-end data pipeline to perform automated and scalable ETL flows using Apache Airflow. At the end of the project, the data pipeline needs to meet at least the criteria below:

1. The data pipeline must run without errors, processing all the expected ETL steps
2. Creation of custom operators to perform specific tasks
3. Assessment and assurance of Data Quality

# Motivation

Continuing the progress of previous projects, a music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow. Using the data engineer concepts learned so far, students are challenged to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. Moreover, the company noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

# Dataset 

We use the same dataset from previous projects, that is residing in a s3 bucket. Here is the S3 link for all the files

* s3a://udacity-dend/

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset we will be working with are partitioned by year and month. 

## Schema Flows

The database schema is the same from the previous projects. Click [here](https://github.com/michelmf/data_engineer_nd/tree/master/Data_Modelling_with_PostgreSQL) to check the schema picture.

The DAG we must create in this project is depicted below. 

![](https://github.com/michelmf/data_engineer_nd/blob/master/Data_Pipelines_With_Airflow/example-dag.png?raw=true)

The DAG steps in this project are:

* Begin execution: A dummy task used to indicate the start of the flow
* Stage_events/Stage_songs: Tasks used to preprocess
* Load_song/user/artist/time_table: Tasks used to load data from fact table to dimension tables
* Run_Data_quality_checks: Task used to assess data quality, such as number of loaded rows, ....
* End_execution: A dummy task to indicate the end of the dag execution

## Files

The project follows the structure below.


# How to run
