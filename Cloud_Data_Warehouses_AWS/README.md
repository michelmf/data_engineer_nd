# Data Warehouses on AWS

This is the third project of the Data Engineer Nanodegree. In this project, students have to develop a data warehouse solution on Amazon Web Services Cloud. During the developmento of the project, student have opportunity to practice the following concepts and technologies:

* Extract, Transform and Load concepts - ETL
* Infrastructure as a Code using Boto3 and Python
* Extraction of data from Amazon S3 Buckets
* Development of databases using Amazon Redshift
* CRUD Operations on Amazon Redshift
* Creation and management of IAM roles and groups

# Motivation

In this project, a music streaming startup called Sparkify has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that **extracts their data from S3**, **stages them in Redshift** and **transforms data into a set of dimensional tables** for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

# Project Goals

In this project, we have to build an ETL pipeline for a database hosted on Redshift. To complete the project, the following tasks must be completed:

1. Load Json files from S3 buckets
2. Stage data in tables on Redshift for Transformation process
3. Insert the transformed data into final tables in Redshift

# Dataset 

We use the same dataset from previous projects, now residing in a s3 bucket. Here are the S3 links for each:

* Song data: s3://udacity-dend/song_data
* Log data: s3://udacity-dend/log_data
* Log data json path: s3://udacity-dend/log_json_path.json

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset you'll be working with are partitioned by year and month. 

## Schema

The schema is the same from the previous projects. Click [here](https://github.com/michelmf/data_engineer_nd/tree/master/Data_Modelling_with_PostgreSQL) to check the schema picture.

## Files

The project follows the structure depicted below.

* Analytics
    * sparkify_analytics : Notebook with some analyzes of data (not finished)

* database
    * create_tables.py : script that executes queries needed to create the tables in redsfhit
    * sql_queries.py : file that contains all the queries from the project

* etl
    * etl.py : Script that executes the ETL process of this project

* iac
    * redshift_cluster.py : Script that implements the Infrastructure as a Code on AWS

* dwh.cfg : Configuration file where all informations from AWS are stored

* main.py : Main script that triggers all the processes of this project

# How to run

The steps needed to create and populate the tables are:

* Open the terminal and execute the redshift_cluster.py script (python iac/create_tables.py)
* Execute the script above, you should execute the main.py script (python etl.py)
* After finishing all the process, you must delete the cluster on AWS console by hand

# Requirements

These packages were used to build this project:

* boto3:    1.12.39
* botocore: 1.15.39
* psycopg2: 2.8.5