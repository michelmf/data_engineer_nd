# Udacity's Data Engineer Nanodegree

In this course, students learn how to design data models, build data warehouses and data lakes, automate data pipelines, and work with massive datasets. At the end of the program, students must combine these new skills by completing a capstone project.

Skills Developed:

* Dimensional Modeling of databases
* SQL and NoSQL data modeling
* ETL Techniques and strategies
* OLAP CUBES
* Data Flows
* Python and SQL Programming
* Creation and Automation of Data Pipeline

Technologies used in this nanodegree:

* PostgreSQL
* Apache Cassandra
* Amazon Web Services (IAM, Redshift, S3, )
* Apache Spark using PySpark
* Airflow

In the sections below I briefly describe each project I developed during the course.

## Project 1 - Data modeling using PostgreSQL

### Description

## Project 2 - Data modeling using Apacha Cassandra

### Description

## Project 3 - Cloud Data Warehouses with AWS

### Description

## Project 4 - Data Lakes with Spark

### Description

## Project 5 - Data Pipelines With AirFlow

### Description

## Project 6 - Capstone Project

### Description
