# Udacity's Data Engineer Nanodegree

In this course, students learn how to design data models, build data warehouses and data lakes, automate data pipelines, and work with massive datasets. At the end of the program, students must combine these new skills by completing a capstone project.

Skills Developed:

* Dimensional Modeling of databases
* SQL and NoSQL data modeling
* ETL Techniques and strategies
* OLAP CUBES
* Data Flows
* Python and SQL Programming
* Creation and Automation of Data Pipeline

Technologies used in this nanodegree:

* PostgreSQL
* Apache Cassandra
* Amazon Web Services (IAM, Redshift, S3, )
* Apache Spark using PySpark
* Airflow

In the sections below I briefly describe each technology and project I developed during the course.

## Section 1 - Data modeling using PostgreSQL and Apache Cassandra

This is the section of udacity Data Engineering Nanodegree, where students have the opportunity to practice the following concepts learned during the classes:

* Data modeling
* Database Schemas (snowflake/star)
* Creation of ETL pipelines
* Database CRUD

This section have two modeling projects.

The first project involves a creation of a PostgreSQL database design to help a fake startup called Sparkify to analyze data from their product, a music streaming app. For more informations about the project, click [here](https://github.com/michelmf/data_engineer_nd/tree/master/Data_Modelling_with_PostgreSQL).

The second project uses a different approach from the first one, where students have to model the app database using NoSQL.  

## Section 2 - Cloud Data Warehouses with AWS

### Description

## Section 3 - Data Lakes with Spark

### Description

## Section 4 - Data Pipelines With AirFlow

### Description

## Section 5 - Capstone Project

